# ðŸ§  LLM Internal Monologue on Raspberry Pi Zero

This project runs on a Raspberry Pi Zero and creates an AI-powered internal monologue by capturing photos, sending them to OpenAI's GPT-4o vision model, and speaking the result using gTTS and pygame.

"mono sex"

...è¨€èªžà¤¬"ÙƒÙ„Ø§Ù…ÑÐ»Ð¾Ð²Ð¾ë§à¸„à¸³parolaðŸ—£ðŸ’¬ðŸ“èªžè¨€Ù„ØºØ©@â€œë§ì”€è¨€è‘‰à¤¶à¤¬à¥à¤¦paroleÎ£PEECHèªžâ‹¯â˜¸ï¸âœï¸â˜¦ï¸â˜ªï¸ðŸ•‰ï¸âœ¡ï¸â˜¯ï¸åð“‚€á›‰âŒ˜ð“Š½â´°ðŸ•Šï¸ð¤€âµ£èžã„ã¦ãã ã•ã„ Ø§Ø³Ù…Ø¹Ù†ÙŠ å¬æˆ‘èªª ÑÐ»ÑƒÑˆÐ°Ð¹ ã‚ˆãèžã‘ Ã©coute-moi à¤¸à¥à¤¨à¥‹ à¤¸à¥à¤¨à¤¿à¤ à¤§à¥à¤¯à¤¾à¤¨ à¤¦à¥‡à¤‚ hark nisikilize lalela ×©×Ö°×žÖ·×¢ audi me pakinggan mo ako whakarongo mai tusarnaarni Î›ÎŸÎ“ÎŸÎ£ Verbum NÄda VÄk â€œI AMâ€ â™ª â™« ð„ž Ê˜ ~ âˆž Ã¸ ðŸ“¢ âŽ‹ # ðŸ§  âš¡ï¸ðŽ€ ð¤€ ðŽ˜ðŽšðŽ— ð°´ð°£ð°† ð¡€ð¡”ð¡Œð¡ - ðŸ’¤

---

## ðŸ“· Features

- Captures an image every few seconds using `libcamera-still`
- Encodes the image and sends it to OpenAI with a reflective prompt
- Converts the GPT-4o response to speech and plays it aloud
- Works fully offline (except for OpenAI API calls)

---

## ðŸ—‚ Directory Structure

```
.
â”œâ”€â”€ eye/
â”‚   â”œâ”€â”€ setup.sh           # Automates setup of venv and dependencies
â”‚   â””â”€â”€ monologue.py       # Main Python script
â”œâ”€â”€ llm-monologue/
â”‚   â”œâ”€â”€ ve/                # Virtual environment (auto-generated)
â”‚   â””â”€â”€ photos/            # Stores captured images
â””â”€â”€ README.md
```

---

## ðŸš€ Quick Start

### ðŸ“¦ Prerequisites

- Raspberry Pi Zero W / Zero 2 W
- Raspberry Pi OS (Lite or Full)
- `libcamera-still` installed
- Working speaker connected to audio output

---

### ðŸ”§ 1. Clone the repository

```bash
git clone https://github.com/yourusername/llm-internal-monologue.git
cd llm-internal-monologue/eye
```

---

### ðŸ”§ 2. Run the setup script

```bash
chmod +x setup.sh
./setup.sh
```

This creates:
- A virtual environment
- A photo directory
- A `requirements.txt`
- Installs `openai`, `gTTS`, `pygame`

---

### ðŸ”§ 3. Activate the environment and run the script

```bash
source /home/zero/llm-monologue/ve/bin/activate
python monologue.py
```

---

## ðŸ”‘ Setting the OpenAI API Key

### One-time:
```bash
export OPENAI_API_KEY="your_key_here"
```

### Persistent (recommended):
Add this to `~/.bashrc` or `~/.profile`:

```bash
echo 'export OPENAI_API_KEY="your_key_here"' >> ~/.bashrc
source ~/.bashrc
```

---

## ðŸ§¹ Optional Cleanup Logic

To prevent storage overflow:
- Overwrite the same image each time (`current.jpg`)
- Or limit history (delete old photos) using cleanup logic in Python

---

## ðŸ’¡ Ideas for Extensions

- Multi-agent monologue system

- Step-by-step (chain-of-thought) reasoning

- Function calling and action triggers

- Memory for past context

- Emotionally expressive voice output

- Faster, offline-compatible performance

- Visual feedback and interface

- Cognitive architecture for modular thinking

- User feedback integration for improvement
---

## ðŸ“„ License

MIT License. See `LICENSE` file.

---

# ðŸ›  Step-by-Step Setup Guide

## 1. Hardware Setup

- Raspberry Pi Zero W or Zero 2 W
- Camera module (connected via CSI port)
- Speakers or 3.5mm audio output
- MicroSD card with Raspberry Pi OS

## 2. Enable SSH (headless setup)

Add an empty `ssh` file to the `/boot` partition of the SD card.

## 3. Wi-Fi (optional)

Also add a `wpa_supplicant.conf` file to connect automatically.

## 4. Clone the repo

```bash
git clone https://github.com/yourusername/llm-internal-monologue.git
```

## 5. Run setup

```bash
cd llm-internal-monologue/eye
chmod +x setup.sh
./setup.sh
```

## 6. Activate and run

```bash
source /home/zero/llm-monologue/ve/bin/activate
python monologue.py
```

## 7. Add API key

```bash
export OPENAI_API_KEY="sk-...."
```

Or store it in `~/.bashrc`.

## 8. Done!

The device will now:
- Take a photo
- Ask OpenAI what it "thinks"
- Say the answer aloud

Repeat.

(The last line is a purity seal)
